# astrbot_plugin_GroupFS/main.py

# è¯·ç¡®ä¿å·²å®‰è£…ä¾èµ–: pip install croniter aiohttp chardet apscheduler
import asyncio
import os
import time
from datetime import datetime
from typing import List, Dict, Optional
import chardet
import subprocess
from apscheduler.schedulers.asyncio import AsyncIOScheduler

import aiohttp
import croniter

from astrbot.api.event import filter, AstrMessageEvent, MessageChain
from astrbot.api.star import Context, Star, register, StarTools
from astrbot.api import logger
import astrbot.api.message_components as Comp
from astrbot.api.message_components import Plain, Node, Nodes
from astrbot.core.platform.sources.aiocqhttp.aiocqhttp_message_event import AiocqhttpMessageEvent
from aiocqhttp.exceptions import ActionFailed 

from . import utils

@register(
    "astrbot_plugin_GroupFS",
    "Foolllll",
    "ç®¡ç†QQç¾¤æ–‡ä»¶",
    "0.9",
    "https://github.com/Foolllll-J/astrbot_plugin_GroupFS"
)
class GroupFSPlugin(Star):
    def __init__(self, context: Context, config: Optional[Dict] = None):
        super().__init__(context)
        self.config = config if config else {}
        self.group_whitelist: List[int] = [int(g) for g in self.config.get("group_whitelist", [])]
        self.admin_users: List[int] = [int(u) for u in self.config.get("admin_users", [])]
        self.preview_length: int = self.config.get("preview_length", 300)
        self.storage_limits: Dict[int, Dict] = {}
        self.cron_configs = []
        self.bot = None
        self.forward_threshold: int = self.config.get("forward_threshold", 0)
        self.scheduler: Optional[AsyncIOScheduler] = None
        
        self.active_tasks = [] 
        
        self.enable_zip_preview: bool = self.config.get("enable_zip_preview", False)
        self.default_zip_password: str = self.config.get("default_zip_password", "")
        self.download_semaphore = asyncio.Semaphore(5)
        
        self.scheduled_autodelete: bool = self.config.get("scheduled_autodelete", False)

        limit_configs = self.config.get("storage_limits", [])
        for item in limit_configs:
            try:
                group_id_str, count_limit_str, space_limit_str = item.split(':')
                group_id = int(group_id_str)
                self.storage_limits[group_id] = { "count_limit": int(count_limit_str), "space_limit_gb": float(space_limit_str) }
            except ValueError as e:
                logger.error(f"è§£æ storage_limits é…ç½® '{item}' æ—¶å‡ºé”™: {e}ï¼Œå·²è·³è¿‡ã€‚")
        
        self.backup_zip_password: str = self.config.get("backup_zip_password", "")
        self.backup_file_size_limit_mb: int = self.config.get("backup_file_size_limit_mb", 0)
        ext_str: str = self.config.get("backup_file_extensions", "txt,zip")
        
        self.backup_file_extensions: List[str] = [
            ext.strip().lstrip('.').lower()
            for ext in ext_str.split(',') 
            if ext.strip()
        ]

        cron_configs = self.config.get("scheduled_check_tasks", [])
        seen_tasks = set()
        for item in cron_configs:
            try:
                group_id_str, cron_str = item.split(':', 1)
                group_id = int(group_id_str)
                if not croniter.croniter.is_valid(cron_str):
                    raise ValueError(f"æ— æ•ˆçš„ cron è¡¨è¾¾å¼: {cron_str}")
                
                task_identifier = (group_id, cron_str)
                if task_identifier in seen_tasks:
                    logger.warning(f"æ£€æµ‹åˆ°é‡å¤çš„å®šæ—¶ä»»åŠ¡é…ç½® '{item}'ï¼Œå·²è·³è¿‡ã€‚")
                    continue
                
                self.cron_configs.append({"group_id": group_id, "cron_str": cron_str})
                seen_tasks.add(task_identifier)
            except ValueError as e:
                logger.error(f"è§£æ scheduled_check_tasks é…ç½® '{item}' æ—¶å‡ºé”™: {e}ï¼Œå·²è·³è¿‡ã€‚")
        
        logger.info("æ’ä»¶ [ç¾¤æ–‡ä»¶ç³»ç»ŸGroupFS] å·²åŠ è½½ã€‚")

    async def initialize(self):
        if self.cron_configs:
            logger.info("[å®šæ—¶ä»»åŠ¡] å¯åŠ¨å¤±æ•ˆæ–‡ä»¶æ£€æŸ¥è°ƒåº¦å™¨...")
            self.scheduler = AsyncIOScheduler()
            self._register_jobs()
            self.scheduler.start()

    def _register_jobs(self):
        """æ ¹æ®é…ç½®æ³¨å†Œå®šæ—¶ä»»åŠ¡"""
        for job_config in self.cron_configs:
            group_id = job_config["group_id"]
            cron_str = job_config["cron_str"]
            job_id = f"scheduled_check_{group_id}_{cron_str.replace(' ', '_')}"
            
            if self.scheduler.get_job(job_id):
                logger.warning(f"ä»»åŠ¡ {job_id} å·²å­˜åœ¨ï¼Œè·³è¿‡æ³¨å†Œã€‚")
                continue
            
            try:
                cron_parts = cron_str.split()
                minute, hour, day, month, day_of_week = cron_parts
                
                self.scheduler.add_job(
                    self._perform_scheduled_check,
                    "cron",
                    args=[group_id, self.scheduled_autodelete],
                    minute=minute,
                    hour=hour,
                    day=day,
                    month=month,
                    day_of_week=day_of_week,
                    id=job_id
                )
                logger.info(f"æˆåŠŸæ³¨å†Œå®šæ—¶ä»»åŠ¡: group_id={group_id}, cron_str='{cron_str}'")
            except Exception as e:
                logger.error(f"æ³¨å†Œå®šæ—¶ä»»åŠ¡ '{cron_str}' å¤±è´¥: {e}", exc_info=True)

    def _split_text_by_length(self, text: str, max_length: int = 1000) -> List[str]:
            """
            å°†æ–‡æœ¬æŒ‰æŒ‡å®šé•¿åº¦åˆ†å‰²æˆä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ã€‚
            """
            return [text[i:i + max_length] for i in range(0, len(text), max_length)]

    async def _send_or_forward(self, event: AstrMessageEvent, text: str, name: str = "GroupFS"):
        total_length = len(text)
        group_id = event.get_group_id()

        if self.forward_threshold > 0 and total_length > self.forward_threshold:
            logger.info(f"[{group_id}] æ£€æµ‹åˆ°é•¿æ¶ˆæ¯ (é•¿åº¦: {total_length} > {self.forward_threshold})ï¼Œå‡†å¤‡è‡ªåŠ¨åˆå¹¶è½¬å‘ã€‚")
            try:
                split_texts = self._split_text_by_length(text, 4000)
                forward_nodes = []
                
                logger.info(f"[{group_id}] å°†æ¶ˆæ¯åˆ†å‰²ä¸º {len(split_texts)} ä¸ªèŠ‚ç‚¹ã€‚")
                for i, part_text in enumerate(split_texts):
                    node_name = f"{name} ({i+1})" if len(split_texts) > 1 else name
                    forward_nodes.append(Node(uin=event.get_self_id(), name=node_name, content=[Plain(part_text)]))

                merged_forward_message = Nodes(nodes=forward_nodes)
                await event.send(MessageChain([merged_forward_message]))
                logger.info(f"[{group_id}] æˆåŠŸå‘é€åˆå¹¶è½¬å‘æ¶ˆæ¯ã€‚")

            except Exception as e:
                logger.error(f"[{group_id}] åˆå¹¶è½¬å‘é•¿æ¶ˆæ¯æ—¶å‡ºé”™: {e}", exc_info=True)
                
                fallback_text = text[:self.forward_threshold] + "... (æ¶ˆæ¯è¿‡é•¿ä¸”åˆå¹¶è½¬å‘å¤±è´¥)"
                await event.send(MessageChain([Comp.Plain(fallback_text)]))
                logger.info(f"[{group_id}] åˆå¹¶è½¬å‘å¤±è´¥ï¼Œå·²å›é€€ä¸ºå‘é€æˆªæ–­çš„æ™®é€šæ¶ˆæ¯ã€‚")
        else:
            logger.info(f"[{group_id}] æ¶ˆæ¯é•¿åº¦æœªè¾¾é˜ˆå€¼ ({total_length} <= {self.forward_threshold})ï¼Œç›´æ¥å‘é€æ™®é€šæ¶ˆæ¯ã€‚")
            try:
                await event.send(MessageChain([Comp.Plain(text)]))
                logger.info(f"[{group_id}] æˆåŠŸå‘é€æ™®é€šæ¶ˆæ¯ã€‚")
            except Exception as e:
                logger.error(f"[{group_id}] å‘é€æ™®é€šæ¶ˆæ¯æ—¶å‡ºé”™: {e}", exc_info=True)

    async def _perform_scheduled_check(self, group_id: int, auto_delete: bool):
        """ç»Ÿä¸€çš„å®šæ—¶æ£€æŸ¥å‡½æ•°ï¼Œæ ¹æ®auto_deleteå†³å®šæ˜¯å¦åˆ é™¤ã€‚"""
        log_prefix = "[æ¸…ç†ä»»åŠ¡]" if auto_delete else "[æ£€æŸ¥ä»»åŠ¡]"
        report_title = "æ¸…ç†æŠ¥å‘Š" if auto_delete else "æ£€æŸ¥æŠ¥å‘Š"
        
        try:
            if not self.bot:
                logger.warning(f"[{group_id}] {log_prefix} æ— æ³•æ‰§è¡Œï¼Œå› ä¸ºå°šæœªæ•è·åˆ° bot å®ä¾‹ã€‚è¯·å…ˆè§¦å‘ä»»æ„ä¸€æ¬¡æŒ‡ä»¤ã€‚")
                return
            bot = self.bot
            logger.info(f"[{group_id}] {log_prefix} å¼€å§‹è·å–å…¨é‡æ–‡ä»¶åˆ—è¡¨...")
            all_files = await self._get_all_files_recursive_core(group_id, bot)
            total_count = len(all_files)
            logger.info(f"[{group_id}] {log_prefix} è·å–åˆ° {total_count} ä¸ªæ–‡ä»¶ï¼Œå‡†å¤‡åˆ†æ‰¹æ£€æŸ¥ã€‚")
            invalid_files_info = []
            deleted_files = []
            failed_deletions = []
            
            batch_size = 50
            for i in range(0, total_count, batch_size):
                batch = all_files[i:i + batch_size]
                for file_info in batch:
                    file_id = file_info.get("file_id")
                    file_name = file_info.get("file_name", "æœªçŸ¥æ–‡ä»¶å")
                    if not file_id: continue
                    try:
                        await bot.api.call_action('get_group_file_url', group_id=group_id, file_id=file_id)
                    except ActionFailed as e:
                        if e.result.get('retcode') == 1200:
                            invalid_files_info.append(file_info)
                            if auto_delete:
                                logger.warning(f"[{group_id}] {log_prefix} å‘ç°å¤±æ•ˆæ–‡ä»¶ '{file_name}'ï¼Œå°è¯•åˆ é™¤...")
                                try:
                                    delete_result = await bot.api.call_action('delete_group_file', group_id=group_id, file_id=file_id)
                                    is_success = False
                                    if delete_result and delete_result.get('transGroupFileResult', {}).get('result', {}).get('retCode') == 0:
                                        is_success = True
                                    if is_success:
                                        logger.info(f"[{group_id}] {log_prefix} æˆåŠŸåˆ é™¤å¤±æ•ˆæ–‡ä»¶: '{file_name}'")
                                        deleted_files.append(file_name)
                                    else:
                                        logger.error(f"[{group_id}] {log_prefix} åˆ é™¤å¤±æ•ˆæ–‡ä»¶ '{file_name}' å¤±è´¥ï¼ŒAPIæœªè¿”å›æˆåŠŸã€‚")
                                        failed_deletions.append(file_name)
                                except Exception as del_e:
                                    logger.error(f"[{group_id}] {log_prefix} åˆ é™¤å¤±æ•ˆæ–‡ä»¶ '{file_name}' æ—¶å‘ç”Ÿå¼‚å¸¸: {del_e}")
                                    failed_deletions.append(file_name)
                    await asyncio.sleep(0.2)
            
            if not invalid_files_info:
                logger.info(f"[{group_id}] {log_prefix} æ£€æŸ¥å®Œæˆï¼Œæœªå‘ç°å¤±æ•ˆæ–‡ä»¶ã€‚")
                return 

            report_message = f"ğŸš¨ {report_title}\nåœ¨ {total_count} ä¸ªç¾¤æ–‡ä»¶ä¸­ï¼Œ"
            report_message += f"å…±å‘ç° {len(invalid_files_info)} ä¸ªå¤±æ•ˆæ–‡ä»¶ã€‚\n"
            
            if auto_delete:
                report_message += f"\n- æˆåŠŸåˆ é™¤: {len(deleted_files)} ä¸ª"
                if failed_deletions:
                    report_message += f"\n- åˆ é™¤å¤±è´¥: {len(failed_deletions)} ä¸ª"
                if not deleted_files and not failed_deletions:
                     report_message += f"ä½†æœªæˆåŠŸåˆ é™¤ä»»ä½•æ–‡ä»¶ã€‚"
                
                report_message += "\n" + "-" * 20
                for info in invalid_files_info:
                    status = "å·²åˆ é™¤" if info.get('file_name') in deleted_files else "åˆ é™¤å¤±è´¥"
                    folder_name = info.get('parent_folder_name', 'æœªçŸ¥')
                    modify_time = utils.format_timestamp(info.get('modify_time'))
                    report_message += f"\n- {info.get('file_name')} ({status})"
                    report_message += f"\n  (æ–‡ä»¶å¤¹: {folder_name} | æ—¶é—´: {modify_time})"
            else:
                report_message += "\n" + "-" * 20
                for info in invalid_files_info:
                    folder_name = info.get('parent_folder_name', 'æœªçŸ¥')
                    modify_time = utils.format_timestamp(info.get('modify_time'))
                    report_message += f"\n- {info.get('file_name')}"
                    report_message += f"\n  (æ–‡ä»¶å¤¹: {folder_name} | æ—¶é—´: {modify_time})"
                report_message += "\n" + "-" * 20
                report_message += "\nå»ºè®®ç®¡ç†å‘˜ä½¿ç”¨ /cdf æŒ‡ä»¤è¿›è¡Œä¸€é”®æ¸…ç†ã€‚"
            
            logger.info(f"[{group_id}] {log_prefix} æ£€æŸ¥å…¨éƒ¨å®Œæˆï¼Œå‡†å¤‡å‘é€æŠ¥å‘Šã€‚")
            if self.bot:
                await self.bot.api.call_action('send_group_msg', group_id=group_id, message=report_message)
        except Exception as e:
            logger.error(f"[{group_id}] {log_prefix} æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}", exc_info=True)
            if self.bot:
                await self.bot.api.call_action('send_group_msg', group_id=group_id, message="âŒ å®šæ—¶ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå†…éƒ¨é”™è¯¯ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚")


    async def _get_all_files_with_path(self, group_id: int, bot) -> List[Dict]:
        """é€’å½’è·å–æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶è®¡ç®—å…¶åœ¨å¤‡ä»½ç›®å½•ä¸­çš„ç›¸å¯¹è·¯å¾„ã€‚"""
        all_files = []
        # ç»“æ„: (folder_id, folder_name, relative_path)
        folders_to_scan = [(None, "æ ¹ç›®å½•", "")] 
        while folders_to_scan:
            current_folder_id, current_folder_name, current_relative_path = folders_to_scan.pop(0)
            
            try:
                if current_folder_id is None or current_folder_id == '/':
                    result = await bot.api.call_action('get_group_root_files', group_id=group_id, file_count=2000)
                else:
                    result = await bot.api.call_action('get_group_files_by_folder', group_id=group_id, folder_id=current_folder_id, file_count=2000)
                
                if not result: continue
                
                if result.get('files'):
                    for file_info in result['files']:
                        file_info['relative_path'] = os.path.join(current_relative_path, file_info.get('file_name', ''))
                        file_info['size'] = file_info.get('size', 0) # ç¡®ä¿æœ‰ size å­—æ®µ
                        all_files.append(file_info)
                        
                if result.get('folders'):
                    for folder in result['folders']:
                        if folder_id := folder.get('folder_id'):
                            new_relative_path = os.path.join(current_relative_path, folder.get('folder_name', ''))
                            folders_to_scan.append((folder_id, folder.get('folder_name', ''), new_relative_path))
                            
            except Exception as e:
                logger.error(f"[{group_id}-ç¾¤æ–‡ä»¶éå†] é€’å½’è·å–æ–‡ä»¶å¤¹ '{current_folder_name}' å†…å®¹æ—¶å‡ºé”™: {e}")
                continue
        return all_files
    
    async def _get_all_files_recursive_core(self, group_id: int, bot) -> List[Dict]:
        """
        å…¼å®¹ /cdf, /cf, /sf, /df ç­‰æŒ‡ä»¤ã€‚
        """
        all_files_with_path = await self._get_all_files_with_path(group_id, bot)
        for file_info in all_files_with_path:
            path_parts = file_info.get('relative_path', '').split(os.path.sep)
            file_info['parent_folder_name'] = os.path.sep.join(path_parts[:-1]) if len(path_parts) > 1 else 'æ ¹ç›®å½•'
        return all_files_with_path
    
    async def _download_and_save_file(self, group_id: int, file_id: str, file_name: str, file_size: int, relative_path: str, root_dir: str, client) -> bool:
        log_prefix = f"[ç¾¤æ–‡ä»¶å¤‡ä»½-{group_id}-ä¸‹è½½]"
        target_path = os.path.join(root_dir, relative_path)
        os.makedirs(os.path.dirname(target_path), exist_ok=True)

        if os.path.exists(target_path):
            try:
                # æ£€æŸ¥å·²å­˜åœ¨æ–‡ä»¶çš„å¤§å°æ˜¯å¦ä¸ç›®æ ‡æ–‡ä»¶å¤§å°åŒ¹é…
                existing_size = os.path.getsize(target_path)
                if existing_size == file_size:
                    logger.info(f"{log_prefix} æ–‡ä»¶ '{file_name}' å·²å­˜åœ¨ ({utils.format_bytes(file_size)})ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
                    return True
                else:
                    logger.warning(f"{log_prefix} æ–‡ä»¶ '{file_name}' å­˜åœ¨ä½†å¤§å°ä¸åŒ¹é… ({utils.format_bytes(existing_size)} != {utils.format_bytes(file_size)})ï¼Œé‡æ–°ä¸‹è½½ã€‚")
                    os.remove(target_path) # å¤§å°ä¸ä¸€è‡´ï¼Œå…ˆåˆ é™¤å†ä¸‹è½½
            except Exception as e:
                logger.warning(f"{log_prefix} æ£€æŸ¥æ–‡ä»¶ '{file_name}' å¤§å°å¤±è´¥ ({e})ï¼Œå°è¯•é‡æ–°ä¸‹è½½ã€‚")
                try:
                    os.remove(target_path)
                except:
                    pass

        try:
            # 1. è·å–ä¸‹è½½é“¾æ¥
            url_result = await client.api.call_action('get_group_file_url', group_id=group_id, file_id=file_id)
            if not (url_result and url_result.get('url')):
                logger.error(f"{log_prefix} æ— æ³•è·å–æ–‡ä»¶ '{file_name}' çš„ä¸‹è½½é“¾æ¥æˆ–æ–‡ä»¶å·²å¤±æ•ˆã€‚")
                return False
            url = url_result['url']

            # 2. ä¸‹è½½æ–‡ä»¶ï¼Œä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            async with aiohttp.ClientSession() as session:
                async with self.download_semaphore:
                    async with session.get(url, timeout=60) as resp:
                        if resp.status != 200:
                            logger.error(f"{log_prefix} ä¸‹è½½æ–‡ä»¶ '{file_name}' å¤±è´¥ (HTTP: {resp.status})ã€‚")
                            return False
                        
                        # 3. å†™å…¥æ–‡ä»¶ï¼Œæ³¨æ„æ•è· OS å¼‚å¸¸ï¼ˆå¦‚ç£ç›˜ç©ºé—´ä¸è¶³ï¼‰
                        with open(target_path, 'wb') as f:
                            async for chunk in resp.content.iter_chunked(8192):
                                f.write(chunk)
            
            logger.info(f"{log_prefix} æˆåŠŸä¸‹è½½æ–‡ä»¶ '{file_name}' ({utils.format_bytes(file_size)}) åˆ°: {target_path}")
            return True
        except ActionFailed as e:
            # NapCat/NTQQ API è°ƒç”¨å¤±è´¥ï¼Œå¦‚æ–‡ä»¶å·²å¤±æ•ˆ(-134)ç­‰
            logger.warning(f"{log_prefix} ä¸‹è½½æ–‡ä»¶ '{file_name}' å¤±è´¥ (APIé”™è¯¯): {e.message if hasattr(e, 'message') else str(e)}")
            return False
        except FileNotFoundError:
            logger.error(f"{log_prefix} åˆ›å»ºç›®æ ‡æ–‡ä»¶è·¯å¾„å¤±è´¥ (FileNotFoundError)ï¼Œå¯èƒ½ç›®å½•åˆ›å»ºå¤±è´¥ã€‚")
            return False
        except OSError as e:
            logger.error(f"{log_prefix} å†™å…¥æ–‡ä»¶ '{file_name}' æ—¶å‘ç”Ÿ OS é”™è¯¯ (å¯èƒ½ç©ºé—´ä¸è¶³): {e}", exc_info=True)
            return False
        except Exception as e:
            logger.error(f"{log_prefix} ä¸‹è½½æ–‡ä»¶ '{file_name}' æ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}", exc_info=True)
            return False

    async def _cleanup_backup_temp(self, backup_dir: str, zip_path: Optional[str]):
        """å¼‚æ­¥æ¸…ç†å¤‡ä»½ç›®å½•å’Œç”Ÿæˆçš„ ZIP æ–‡ä»¶ã€‚"""
        try:
            await asyncio.sleep(600)  # ç­‰å¾…10åˆ†é’Ÿåå†æ¸…ç†
            if os.path.exists(backup_dir):
                for dirpath, dirnames, filenames in os.walk(backup_dir, topdown=False):
                    for filename in filenames:
                        os.remove(os.path.join(dirpath, filename))
                    for dirname in dirnames:
                        os.rmdir(os.path.join(dirpath, dirname))
                os.rmdir(backup_dir)
                logger.info(f"[ç¾¤æ–‡ä»¶å¤‡ä»½-æ¸…ç†] å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {backup_dir}")
            
            await asyncio.sleep(5)

            # åˆ é™¤ç”Ÿæˆçš„ ZIP æ–‡ä»¶
            if zip_path and os.path.exists(os.path.dirname(zip_path)):
                zip_base_name_no_ext = os.path.basename(zip_path).rsplit('.zip', 1)[0]
                temp_base_dir = os.path.dirname(zip_path)
                
                # éå†æ‰€æœ‰å¯èƒ½çš„åˆ†å·æ–‡ä»¶å¹¶åˆ é™¤
                for f in os.listdir(temp_base_dir):
                    if f.startswith(zip_base_name_no_ext):
                         file_to_delete = os.path.join(temp_base_dir, f)
                         os.remove(file_to_delete)
                         logger.info(f"[ç¾¤æ–‡ä»¶å¤‡ä»½-æ¸…ç†] å·²æ¸…ç†ç”Ÿæˆçš„å‹ç¼©åŒ…/åˆ†å·: {f}")

        except OSError as e:
            logger.warning(f"[ç¾¤æ–‡ä»¶å¤‡ä»½-æ¸…ç†] åˆ é™¤ä¸´æ—¶æ–‡ä»¶æˆ–ç›®å½•å¤±è´¥: {e}")

    async def _upload_and_send_file_via_api(self, event: AstrMessageEvent, file_path: str, file_name: str) -> bool:
        log_prefix = f"[ç¾¤æ–‡ä»¶å¤‡ä»½-ä¸Šä¼ /å‘é€]"
        client = self.bot or event.client
        target_id = int(event.get_sender_id())
        group_id_str = event.get_group_id() 
        file_uri = f"file://{file_path}"
        
        upload_result = None # åˆå§‹åŒ–å˜é‡
        
        try:
            # 1. API Call
            if group_id_str:
                target_group_id = int(group_id_str)
                logger.info(f"{log_prefix} è°ƒç”¨ /upload_group_file ä¸Šä¼ æ–‡ä»¶åˆ°ç¾¤ {target_group_id}")
                upload_result = await client.api.call_action('upload_group_file', 
                                                             group_id=target_group_id,
                                                             file=file_uri,
                                                             name=file_name,
                                                             folder_id='/',
                                                             timeout=300)
                
            else:
                logger.info(f"{log_prefix} è°ƒç”¨ /upload_private_file ä¸Šä¼ æ–‡ä»¶åˆ°ç§èŠ {target_id}")
                upload_result = await client.api.call_action('upload_private_file', 
                                                             user_id=target_id,
                                                             file=file_uri,
                                                             name=file_name,
                                                             timeout=300)
            
            # 2. æ£€æŸ¥ upload_result æ˜¯å¦ä¸º None
            if upload_result is None:
                 logger.warning(f"{log_prefix} æ–‡ä»¶ {file_name} ä¸Šä¼ æ—¶ API è°ƒç”¨è¿”å› NONEã€‚æ ¹æ®æµ‹è¯•ç»éªŒï¼Œæ–‡ä»¶å¯èƒ½å·²åœ¨åå°æäº¤ã€‚")
                 return True # è§†ä¸ºæˆåŠŸå¹¶ç»§ç»­ä¸‹ä¸€ä¸ªåˆ†å·
            
            # 3. æ£€æŸ¥ API å“åº”çŠ¶æ€ï¼šstatus='ok' ä¸” retcode=0 (æ­£å¸¸æˆåŠŸ)
            if upload_result.get('status') == 'ok' and upload_result.get('retcode') == 0:
                logger.info(f"{log_prefix} æ–‡ä»¶ {file_name} ä¸Šä¼ è°ƒç”¨æˆåŠŸã€‚")
                return True
            
            # 4. å¤„ç† API æ˜ç¡®è¿”å›å¤±è´¥çŠ¶æ€
            else:
                error_msg = upload_result.get('wording', upload_result.get('errMsg', 'APIè¿”å›å¤±è´¥'))
                retcode = upload_result.get('retcode')
                
                # å¦‚æœè¿”å›çš„é”™è¯¯æ˜¯ NTQQ çš„ "rich media transfer failed" (retcode=1200)
                if retcode == 1200:
                    logger.error(f"{log_prefix} æ–‡ä»¶ {file_name} ä¸Šä¼ å¤±è´¥ (NTQQå†…éƒ¨æ‹’ç»: {error_msg})ã€‚è§†ä¸ºè‡´å‘½å¤±è´¥ï¼Œä¸­æ–­ä»»åŠ¡ã€‚")
                    # å®¢æˆ·ç«¯æ‹’ç»ï¼Œè¿”å› False
                    return False
                else:
                    # å…¶ä»–é 1200 çš„å¤±è´¥ç ï¼Œæˆ–è€… retcode ä¸º None
                    # æ ¹æ®ç»éªŒï¼Œretcode=None é€šå¸¸è¡¨ç¤ºè¯·æ±‚å·²æäº¤ä½†å“åº”å¼‚å¸¸ï¼Œæ–‡ä»¶å¯èƒ½å·²åœ¨åå°å¤„ç†
                    if retcode is None:
                        logger.info(f"{log_prefix} æ–‡ä»¶ {file_name} ä¸Šä¼ è°ƒç”¨è¿”å›å¼‚å¸¸çŠ¶æ€ (retcode=None)ï¼Œä½†æ ¹æ®ç»éªŒæ–‡ä»¶å¯èƒ½å·²æäº¤æˆåŠŸï¼Œç»§ç»­æ‰§è¡Œã€‚")
                    else:
                        logger.warning(f"{log_prefix} æ–‡ä»¶ {file_name} ä¸Šä¼ è¿”å›éæ ‡å‡†çŠ¶æ€ç  (retcode={retcode})ã€‚è¯¦æƒ…: {error_msg}ã€‚å®¹å¿å¹¶ç»§ç»­ã€‚")
                    return True

        except ActionFailed as e:
            # æ•è· ActionFailed
            logger.warning(f"{log_prefix} æ–‡ä»¶ {file_name} ä¸Šä¼ æ—¶å‘ç”Ÿ ActionFailed (ç½‘ç»œä¸­æ–­/è¶…æ—¶)ã€‚é”™è¯¯: {e}")
            return False # è§†ä¸ºå¤±è´¥ï¼Œä¸­æ–­ä»»åŠ¡
            
        except Exception as e:
            error_type = type(e).__name__
            logger.warning(f"{log_prefix} ä¸Šä¼ æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿ Python è‡´å‘½é”™è¯¯ ({error_type})ã€‚æ ¹æ®æµ‹è¯•ç»éªŒï¼Œæ–‡ä»¶å¯èƒ½å·²æäº¤ã€‚é”™è¯¯: {e}", exc_info=True)
            return False

    @filter.command("cdf")
    async def on_check_and_delete_command(self, event: AstrMessageEvent):
        if not self.bot: self.bot = event.bot
        group_id = int(event.get_group_id())
        user_id = int(event.get_sender_id())
        logger.info(f"[{group_id}] ç”¨æˆ· {user_id} è§¦å‘ /cdf å¤±æ•ˆæ–‡ä»¶æ¸…ç†æŒ‡ä»¤ã€‚")
        if user_id not in self.admin_users:
            await event.send(MessageChain([Comp.Plain("âš ï¸ æ‚¨æ²¡æœ‰æ‰§è¡Œæ­¤æ“ä½œçš„æƒé™ã€‚")]))
            return
        await event.send(MessageChain([Comp.Plain("âš ï¸ è­¦å‘Šï¼šå³å°†å¼€å§‹æ‰«æå¹¶è‡ªåŠ¨åˆ é™¤æ‰€æœ‰å¤±æ•ˆæ–‡ä»¶ï¼\næ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ï¼Œå®Œæˆåå°†å‘é€æŠ¥å‘Šã€‚")]))
        self.active_tasks.append(asyncio.create_task(self._perform_batch_check_and_delete(event)))
        event.stop_event()

    async def _perform_batch_check_and_delete(self, event: AstrMessageEvent):
        group_id = int(event.get_group_id())
        try:
            logger.info(f"[{group_id}] [æ‰¹é‡æ¸…ç†] å¼€å§‹è·å–å…¨é‡æ–‡ä»¶åˆ—è¡¨...")
            all_files = await self._get_all_files_recursive_core(group_id, event.bot)
            total_count = len(all_files)
            logger.info(f"[{group_id}] [æ‰¹é‡æ¸…ç†] è·å–åˆ° {total_count} ä¸ªæ–‡ä»¶ï¼Œå‡†å¤‡åˆ†æ‰¹å¤„ç†ã€‚")
            deleted_files = []
            failed_deletions = []
            checked_count = 0
            batch_size = 50
            for i in range(0, total_count, batch_size):
                batch = all_files[i:i + batch_size]
                logger.info(f"[{group_id}] [æ‰¹é‡æ¸…ç†] æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{ -(-total_count // batch_size)}...")
                for file_info in batch:
                    file_id = file_info.get("file_id")
                    file_name = file_info.get("file_name", "æœªçŸ¥æ–‡ä»¶å")
                    if not file_id: continue
                    is_invalid = False
                    try:
                        await event.bot.api.call_action('get_group_file_url', group_id=group_id, file_id=file_id)
                    except ActionFailed as e:
                        if e.result.get('retcode') == 1200:
                            is_invalid = True
                    if is_invalid:
                        logger.warning(f"[{group_id}] [æ‰¹é‡æ¸…ç†] å‘ç°å¤±æ•ˆæ–‡ä»¶ '{file_name}'ï¼Œå°è¯•åˆ é™¤...")
                        try:
                            delete_result = await event.bot.api.call_action('delete_group_file', group_id=group_id, file_id=file_id)
                            is_success = False
                            if delete_result:
                                trans_result = delete_result.get('transGroupFileResult', {})
                                result_obj = trans_result.get('result', {})
                                if result_obj.get('retCode') == 0:
                                    is_success = True
                            if is_success:
                                logger.info(f"[{group_id}] [æ‰¹é‡æ¸…ç†] æˆåŠŸåˆ é™¤å¤±æ•ˆæ–‡ä»¶: '{file_name}'")
                                deleted_files.append(file_name)
                            else:
                                logger.error(f"[{group_id}] [æ‰¹é‡æ¸…ç†] åˆ é™¤å¤±æ•ˆæ–‡ä»¶ '{file_name}' å¤±è´¥ï¼ŒAPIæœªè¿”å›æˆåŠŸã€‚")
                                failed_deletions.append(file_name)
                        except Exception as del_e:
                            logger.error(f"[{group_id}] [æ‰¹é‡æ¸…ç†] åˆ é™¤å¤±æ•ˆæ–‡ä»¶ '{file_name}' æ—¶å‘ç”Ÿå¼‚å¸¸: {del_e}")
                            failed_deletions.append(file_name)
                    checked_count += 1
                    await asyncio.sleep(0.2)
                logger.info(f"[{group_id}] [æ‰¹é‡æ¸…ç†] æ‰¹æ¬¡å¤„ç†å®Œæ¯•ï¼Œå·²æ£€æŸ¥ {checked_count}/{total_count} ä¸ªæ–‡ä»¶ã€‚")
            report_message = f"âœ… æ¸…ç†å®Œæˆï¼\nå…±æ‰«æäº† {total_count} ä¸ªæ–‡ä»¶ã€‚\n\n"
            if deleted_files:
                report_message += f"æˆåŠŸåˆ é™¤äº† {len(deleted_files)} ä¸ªå¤±æ•ˆæ–‡ä»¶ï¼š\n"
                report_message += "\n".join(f"- {name}" for name in deleted_files)
            else:
                report_message += "æœªå‘ç°æˆ–æœªæˆåŠŸåˆ é™¤ä»»ä½•å¤±æ•ˆæ–‡ä»¶ã€‚"
            if failed_deletions:
                report_message += f"\n\nğŸš¨ æœ‰ {len(failed_deletions)} ä¸ªå¤±æ•ˆæ–‡ä»¶åˆ é™¤å¤±è´¥ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å¤„ç†ï¼š\n"
                report_message += "\n".join(f"- {name}" for name in failed_deletions)
            logger.info(f"[{group_id}] [æ‰¹é‡æ¸…ç†] æ£€æŸ¥å…¨éƒ¨å®Œæˆï¼Œå‡†å¤‡å‘é€æŠ¥å‘Šã€‚")
            await self._send_or_forward(event, report_message, name="å¤±æ•ˆæ–‡ä»¶æ¸…ç†æŠ¥å‘Š")
        except Exception as e:
            logger.error(f"[{group_id}] [æ‰¹é‡æ¸…ç†] æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}", exc_info=True)
            await event.send(MessageChain([Comp.Plain("âŒ åœ¨æ‰§è¡Œæ‰¹é‡æ¸…ç†æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚")]))

    @filter.command("cf")
    async def on_check_files_command(self, event: AstrMessageEvent):
        if not self.bot: self.bot = event.bot
        group_id = int(event.get_group_id())
        user_id = int(event.get_sender_id())
        if user_id not in self.admin_users:
            await event.send(MessageChain([Comp.Plain("âš ï¸ æ‚¨æ²¡æœ‰æ‰§è¡Œæ­¤æ“ä½œçš„æƒé™ã€‚")]))
            return
        logger.info(f"[{group_id}] ç”¨æˆ· {user_id} è§¦å‘ /cf å¤±æ•ˆæ–‡ä»¶æ£€æŸ¥æŒ‡ä»¤ã€‚")
        await event.send(MessageChain([Comp.Plain("âœ… å·²å¼€å§‹æ‰«æç¾¤å†…æ‰€æœ‰æ–‡ä»¶ï¼ŒæŸ¥æ‰¾å¤±æ•ˆæ–‡ä»¶...\nè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚\nå¦‚æœæœªå‘ç°å¤±æ•ˆæ–‡ä»¶ï¼Œå°†ä¸ä¼šå‘é€ä»»ä½•æ¶ˆæ¯ã€‚")]))
        self.active_tasks.append(asyncio.create_task(self._perform_scheduled_check(group_id, False)))
        event.stop_event()
    
    @filter.event_message_type(filter.EventMessageType.GROUP_MESSAGE, priority=10)
    async def on_group_file_upload(self, event: AstrMessageEvent):
        if not self.bot: self.bot = event.bot
        has_file = any(isinstance(seg, Comp.File) for seg in event.get_messages())
        if has_file:
            group_id = int(event.get_group_id())
            logger.info(f"[{group_id}] æ£€æµ‹åˆ°æ–‡ä»¶ä¸Šä¼ äº‹ä»¶ï¼Œå°†åœ¨5ç§’åè§¦å‘å®¹é‡æ£€æŸ¥ã€‚")
            self.active_tasks.append(asyncio.create_task(self._check_storage_and_notify(event)))

    async def _check_storage_and_notify(self, event: AstrMessageEvent):
        group_id = int(event.get_group_id())
        if group_id not in self.storage_limits:
            return
        try:
            client = event.bot
            system_info = await client.api.call_action('get_group_file_system_info', group_id=group_id)
            if not system_info: return
            file_count = system_info.get('file_count', 0)
            used_space_bytes = system_info.get('used_space', 0)
            used_space_gb = float(utils.format_bytes(used_space_bytes, 'GB'))
            limits = self.storage_limits[group_id]
            count_limit = limits['count_limit']
            space_limit = limits['space_limit_gb']
            notifications = []
            if file_count >= count_limit:
                notifications.append(f"æ–‡ä»¶æ•°é‡å·²è¾¾ {file_count}ï¼Œæ¥è¿‘æˆ–è¶…è¿‡è®¾å®šçš„ {count_limit} ä¸Šé™ï¼")
            if used_space_gb >= space_limit:
                notifications.append(f"å·²ç”¨ç©ºé—´å·²è¾¾ {used_space_gb:.2f}GBï¼Œæ¥è¿‘æˆ–è¶…è¿‡è®¾å®šçš„ {space_limit:.2f}GB ä¸Šé™ï¼")
            if notifications:
                full_notification = "âš ï¸ ç¾¤æ–‡ä»¶å®¹é‡è­¦å‘Š âš ï¸\n" + "\n".join(notifications) + "\nè¯·åŠæ—¶æ¸…ç†æ–‡ä»¶ï¼"
                logger.warning(f"[{group_id}] å‘é€å®¹é‡è¶…é™è­¦å‘Š: {full_notification}")
                await event.send(MessageChain([Comp.Plain(full_notification)]))
        except ActionFailed as e:
            logger.error(f"[{group_id}] è°ƒç”¨ get_group_file_system_info å¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"[{group_id}] å¤„ç†å®¹é‡æ£€æŸ¥æ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}", exc_info=True)
    
    def _format_search_results(self, files: List[Dict], search_term: str, for_delete: bool = False) -> str:
        reply_text = f"ğŸ” æ‰¾åˆ°äº† {len(files)} ä¸ªä¸ã€Œ{search_term}ã€ç›¸å…³çš„ç»“æœï¼š\n"
        reply_text += "-" * 20
        for i, file_info in enumerate(files, 1):
            reply_text += (
                f"\n[{i}] {file_info.get('file_name')}"
                f"\n  ä¸Šä¼ è€…: {file_info.get('uploader_name', 'æœªçŸ¥')}"
                f"\n  å¤§å°: {utils.format_bytes(file_info.get('size'))}"
                f"\n  ä¿®æ”¹æ—¶é—´: {utils.format_timestamp(file_info.get('modify_time'))}"
            )
        reply_text += "\n" + "-" * 20
        if for_delete:
            reply_text += f"\nè¯·ä½¿ç”¨ /df {search_term} [åºå·] æ¥åˆ é™¤æŒ‡å®šæ–‡ä»¶ã€‚"
        else:
            reply_text += f"\nå¦‚éœ€åˆ é™¤ï¼Œè¯·ä½¿ç”¨ /df {search_term} [åºå·]"
        return reply_text
    
    @filter.command("sf")
    async def on_search_file_command(self, event: AstrMessageEvent):
        if not self.bot: self.bot = event.bot
        group_id = int(event.get_group_id())
        user_id = int(event.get_sender_id())
        command_parts = event.message_str.split(maxsplit=2)
        if len(command_parts) < 2 or not command_parts[1]:
            await event.send(MessageChain([Comp.Plain("â“ è¯·æä¾›è¦æœç´¢çš„æ–‡ä»¶åã€‚ç”¨æ³•: /sf <æ–‡ä»¶å> [åºå·]")]))
            return
        filename_to_find = command_parts[1]
        index_str = command_parts[2] if len(command_parts) > 2 else None
        logger.info(f"[{group_id}] ç”¨æˆ· {user_id} è§¦å‘ /sf, ç›®æ ‡: '{filename_to_find}', åºå·: {index_str}")
        
        all_files = await self._get_all_files_recursive_core(group_id, event.bot)
        found_files = []
        for file_info in all_files:
            current_filename = file_info.get('file_name', '')
            base_name, _ = os.path.splitext(current_filename)
            if filename_to_find in base_name or filename_to_find in current_filename:
                found_files.append(file_info)
        
        logger.info(f"[{group_id}] åœ¨ {len(all_files)} ä¸ªæ–‡ä»¶ä¸­ï¼Œæ‰¾åˆ° {len(found_files)} ä¸ªåŒ¹é…é¡¹ã€‚")

        if not found_files:
            await event.send(MessageChain([Comp.Plain(f"âŒ æœªåœ¨ç¾¤æ–‡ä»¶ä¸­æ‰¾åˆ°ä¸ã€Œ{filename_to_find}ã€ç›¸å…³çš„ä»»ä½•æ–‡ä»¶ã€‚")]))
            return
        if not index_str:
            reply_text = self._format_search_results(found_files, filename_to_find)
            await self._send_or_forward(event, reply_text, name="æ–‡ä»¶æœç´¢ç»“æœ")
            return
        try:
            index = int(index_str)
            if not (1 <= index <= len(found_files)):
                await event.send(MessageChain([Comp.Plain(f"âŒ åºå·é”™è¯¯ï¼æ‰¾åˆ°äº† {len(found_files)} ä¸ªæ–‡ä»¶ï¼Œè¯·è¾“å…¥ 1 åˆ° {len(found_files)} ä¹‹é—´çš„æ•°å­—ã€‚")]))
                return
            file_to_preview = found_files[index - 1]
            preview_text, error_msg = await self._get_file_preview(event, file_to_preview)
            if error_msg:
                await event.send(MessageChain([Comp.Plain(error_msg)]))
                return
            
            reply_text = (
                f"ğŸ“„ æ–‡ä»¶ã€Œ{file_to_preview.get('file_name')}ã€å†…å®¹é¢„è§ˆï¼š\n"
                + "-" * 20 + "\n"
                + preview_text
            )
            await self._send_or_forward(event, reply_text, name=f"æ–‡ä»¶é¢„è§ˆï¼š{file_to_preview.get('file_name')}")
        except ValueError:
            await event.send(MessageChain([Comp.Plain("âŒ åºå·å¿…é¡»æ˜¯ä¸€ä¸ªæ•°å­—ã€‚")]))
        except Exception as e:
            logger.error(f"[{group_id}] å¤„ç†é¢„è§ˆæ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}", exc_info=True)
            await event.send(MessageChain([Comp.Plain("âŒ é¢„è§ˆæ–‡ä»¶æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚")]))
            
    @filter.command("df")
    async def on_delete_file_command(self, event: AstrMessageEvent):
        if not self.bot: self.bot = event.bot
        group_id = int(event.get_group_id())
        user_id = int(event.get_sender_id())
        command_parts = event.message_str.split(maxsplit=2)
        if len(command_parts) < 2 or not command_parts[1]:
            await event.send(MessageChain([Comp.Plain("â“ è¯·æä¾›è¦åˆ é™¤çš„æ–‡ä»¶åã€‚ç”¨æ³•: /df <æ–‡ä»¶å> [åºå·]")]))
            return
        filename_to_find = command_parts[1]
        index_str = command_parts[2] if len(command_parts) > 2 else None
        logger.info(f"[{group_id}] ç”¨æˆ· {user_id} è§¦å‘åˆ é™¤æŒ‡ä»¤ /df, ç›®æ ‡: '{filename_to_find}', åºå·: {index_str}")
        if user_id not in self.admin_users:
            await event.send(MessageChain([Comp.Plain("âš ï¸ æ‚¨æ²¡æœ‰æ‰§è¡Œæ­¤æ“ä½œçš„æƒé™ã€‚")]))
            return

        all_files = await self._get_all_files_recursive_core(group_id, event.bot)
        found_files = []
        for file_info in all_files:
            current_filename = file_info.get('file_name', '')
            base_name, _ = os.path.splitext(current_filename)
            if filename_to_find in base_name or filename_to_find in current_filename:
                found_files.append(file_info)

        logger.info(f"[{group_id}] åœ¨ {len(all_files)} ä¸ªæ–‡ä»¶ä¸­ï¼Œæ‰¾åˆ° {len(found_files)} ä¸ªåŒ¹é…é¡¹ç”¨äºåˆ é™¤ã€‚")
            
        if not found_files:
            await event.send(MessageChain([Comp.Plain(f"âŒ æœªæ‰¾åˆ°ä¸ã€Œ{filename_to_find}ã€ç›¸å…³çš„ä»»ä½•æ–‡ä»¶ã€‚")]))
            return
            
        if index_str == '0':
            self.active_tasks.append(asyncio.create_task(self._perform_batch_delete(event, found_files)))
            event.stop_event()
            return

        file_to_delete = None
        if len(found_files) == 1 and not index_str:
            file_to_delete = found_files[0]
        elif index_str:
            try:
                index = int(index_str)
                if 1 <= index <= len(found_files):
                    file_to_delete = found_files[index - 1]
                else:
                    await event.send(MessageChain([Comp.Plain(f"âŒ åºå·é”™è¯¯ï¼æ‰¾åˆ°äº† {len(found_files)} ä¸ªæ–‡ä»¶ï¼Œè¯·è¾“å…¥ 1 åˆ° {len(found_files)} ä¹‹é—´çš„æ•°å­—ã€‚")]))
                    return
            except ValueError:
                await event.send(MessageChain([Comp.Plain("âŒ åºå·å¿…é¡»æ˜¯ä¸€ä¸ªæ•°å­—ã€‚")]))
                return
        else:
            reply_text = self._format_search_results(found_files, filename_to_find, for_delete=True)
            await self._send_or_forward(event, reply_text, name="æ–‡ä»¶æœç´¢ç»“æœ")
            return

        if not file_to_delete:
            await event.send(MessageChain([Comp.Plain("âŒ å†…éƒ¨é”™è¯¯ï¼Œæœªèƒ½ç¡®å®šè¦åˆ é™¤çš„æ–‡ä»¶ã€‚")]))
            return
        try:
            file_id_to_delete = file_to_delete.get("file_id")
            found_filename = file_to_delete.get("file_name")
            if not file_id_to_delete:
                await event.send(MessageChain([Comp.Plain(f"âŒ æ‰¾åˆ°æ–‡ä»¶ã€Œ{found_filename}ã€ï¼Œä½†æ— æ³•è·å–å…¶IDï¼Œåˆ é™¤å¤±è´¥ã€‚")]))
                return
            logger.info(f"[{group_id}] ç¡®è®¤åˆ é™¤æ–‡ä»¶ '{found_filename}', File ID: {file_id_to_delete}...")
            client = event.bot
            delete_result = await client.api.call_action('delete_group_file', group_id=group_id, file_id=file_id_to_delete)
            is_success = False
            if delete_result:
                trans_result = delete_result.get('transGroupFileResult', {})
                result_obj = trans_result.get('result', {})
                if result_obj.get('retCode') == 0:
                    is_success = True
            if is_success:
                await event.send(MessageChain([Comp.Plain(f"âœ… æ–‡ä»¶ã€Œ{found_filename}ã€å·²æˆåŠŸåˆ é™¤ã€‚")]))
                logger.info(f"[{group_id}] æ–‡ä»¶ '{found_filename}' å·²æˆåŠŸåˆ é™¤ã€‚")
            else:
                error_msg = delete_result.get('wording', 'APIæœªè¿”å›æˆåŠŸçŠ¶æ€')
                await event.send(MessageChain([Comp.Plain(f"âŒ åˆ é™¤æ–‡ä»¶ã€Œ{found_filename}ã€å¤±è´¥: {error_msg}")]))
        except Exception as e:
            logger.error(f"[{group_id}] å¤„ç†åˆ é™¤æµç¨‹æ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}", exc_info=True)
            await event.send(MessageChain([Comp.Plain(f"âŒ å¤„ç†åˆ é™¤æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚")]))

    async def _perform_batch_delete(self, event: AstrMessageEvent, files_to_delete: List[Dict]):
        group_id = int(event.get_group_id())
        deleted_files = []
        failed_deletions = []
        total_count = len(files_to_delete)
        logger.info(f"[{group_id}] [æ‰¹é‡åˆ é™¤] å¼€å§‹å¤„ç† {total_count} ä¸ªæ–‡ä»¶çš„åˆ é™¤ä»»åŠ¡ã€‚")
        for i, file_info in enumerate(files_to_delete):
            file_id = file_info.get("file_id")
            file_name = file_info.get("file_name", "æœªçŸ¥æ–‡ä»¶å")
            if not file_id:
                failed_deletions.append(f"{file_name} (ç¼ºå°‘File ID)")
                continue
            try:
                logger.info(f"[{group_id}] [æ‰¹é‡åˆ é™¤] ({i+1}/{total_count}) æ­£åœ¨åˆ é™¤ '{file_name}'...")
                delete_result = await event.bot.api.call_action('delete_group_file', group_id=group_id, file_id=file_id)
                is_success = False
                if delete_result:
                    trans_result = delete_result.get('transGroupFileResult', {})
                    result_obj = trans_result.get('result', {})
                    if result_obj.get('retCode') == 0:
                        is_success = True
                if is_success:
                    deleted_files.append(file_name)
                else:
                    failed_deletions.append(file_name)
            except Exception as e:
                logger.error(f"[{group_id}] [æ‰¹é‡åˆ é™¤] åˆ é™¤ '{file_name}' æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                failed_deletions.append(file_name)
            await asyncio.sleep(0.5)
        report_message = f"âœ… æ‰¹é‡åˆ é™¤å®Œæˆï¼\nå…±å¤„ç†äº† {total_count} ä¸ªæ–‡ä»¶ã€‚\n\n"
        if deleted_files:
            report_message += f"æˆåŠŸåˆ é™¤äº† {len(deleted_files)} ä¸ªå¤±æ•ˆæ–‡ä»¶ï¼š\n"
            report_message += "\n".join(f"- {name}" for name in deleted_files)
        else:
            report_message += "æœªèƒ½æˆåŠŸåˆ é™¤ä»»ä½•æ–‡ä»¶ã€‚"
        if failed_deletions:
            report_message += f"\n\nğŸš¨ æœ‰ {len(failed_deletions)} ä¸ªæ–‡ä»¶åˆ é™¤å¤±è´¥ï¼š\n"
            report_message += "\n".join(f"- {name}" for name in failed_deletions)
        logger.info(f"[{group_id}] [æ‰¹é‡åˆ é™¤] ä»»åŠ¡å®Œæˆï¼Œå‡†å¤‡å‘é€æŠ¥å‘Šã€‚")
        await self._send_or_forward(event, report_message, name="æ‰¹é‡åˆ é™¤æŠ¥å‘Š")

    def _get_preview_from_bytes(self, content_bytes: bytes) -> tuple[str, str]:
        """ä»å­—èŠ‚å†…å®¹ä¸­å°è¯•è·å–æ–‡æœ¬é¢„è§ˆå’Œç¼–ç ã€‚"""
        try:
            detection = chardet.detect(content_bytes)
            encoding = detection.get('encoding', 'utf-8') or 'utf-8'
            if encoding and detection['confidence'] > 0.7:
                decoded_text = content_bytes.decode(encoding, errors='ignore').strip()
                return decoded_text, encoding
            return "", "æœªçŸ¥"
        except Exception:
            return "", "æœªçŸ¥"

    async def _get_preview_from_zip(self, file_path: str) -> tuple[str, str]:
        """ä»æœ¬åœ°å‹ç¼©æ–‡ä»¶ä¸­è§£å‹å¹¶é¢„è§ˆç¬¬ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ã€‚è¿”å› (é¢„è§ˆå†…å®¹, é”™è¯¯ä¿¡æ¯)ã€‚
           ä½¿ç”¨ 7za å‘½ä»¤æ¥æ”¯æŒæ›´å¤šæ ¼å¼ã€‚
        """
        temp_dir = os.path.join(StarTools.get_data_dir('astrbot_plugin_GroupFS'), 'temp_file_previews')
        os.makedirs(temp_dir, exist_ok=True)
        extract_path = os.path.join(temp_dir, f"extract_{int(time.time())}")
        os.makedirs(extract_path, exist_ok=True)
        
        preview_text = ""
        error_msg = None
        
        try:
            logger.info(f"æ­£åœ¨å°è¯•æ— å¯†ç è§£å‹æ–‡ä»¶ '{os.path.basename(file_path)}'...")
            command_no_pwd = ["7za", "x", file_path, f"-o{extract_path}", "-y"]
            process = await asyncio.create_subprocess_exec(
                *command_no_pwd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                if self.default_zip_password:
                    logger.info("æ— å¯†ç è§£å‹å¤±è´¥ï¼Œæ­£åœ¨å°è¯•ä½¿ç”¨é»˜è®¤å¯†ç ...")
                    command_with_pwd = ["7za", "x", file_path, f"-o{extract_path}", f"-p{self.default_zip_password}", "-y"]
                    process = await asyncio.create_subprocess_exec(
                        *command_with_pwd,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE
                    )
                    stdout, stderr = await process.communicate()
                    
                    if process.returncode != 0:
                        error_msg = stderr.decode('utf-8').strip()
                        logger.error(f"ä½¿ç”¨é»˜è®¤å¯†ç è§£å‹å¤±è´¥: {error_msg}")
                        error_msg = "è§£å‹å¤±è´¥ï¼Œå¯èƒ½å¯†ç ä¸æ­£ç¡®"
                else:
                    error_msg = stderr.decode('utf-8').strip()
                    logger.error(f"ä½¿ç”¨ 7za å‘½ä»¤è§£å‹å¤±è´¥ä¸”æœªè®¾ç½®é»˜è®¤å¯†ç : {error_msg}")
                    error_msg = "è§£å‹å¤±è´¥ï¼Œå¯èƒ½æ–‡ä»¶å·²åŠ å¯†"
            
            if error_msg:
                return "", error_msg

            all_extracted_files = [os.path.join(dirpath, f) for dirpath, _, filenames in os.walk(extract_path) for f in filenames]
            preview_file_path = None
            
            for f_path in all_extracted_files:
                if f_path.lower().endswith('.txt'):
                    preview_file_path = f_path
                    break
            
            if not preview_file_path:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°txtæ–‡ä»¶ï¼Œè¾“å‡ºå‹ç¼©åŒ…çš„æ–‡ä»¶ç»“æ„
                if not all_extracted_files:
                    return "", "å‹ç¼©åŒ…ä¸ºç©ºæˆ–è§£å‹å¤±è´¥"
                
                # æ„å»ºæ–‡ä»¶ç»“æ„æ ‘
                file_structure = ["ğŸ“¦ å‹ç¼©åŒ…å†…æ–‡ä»¶ç»“æ„ï¼š\n"]
                for f_path in sorted(all_extracted_files):
                    relative_path = os.path.relpath(f_path, extract_path)
                    file_size = os.path.getsize(f_path)
                    size_str = utils.format_bytes(file_size)
                    # è®¡ç®—ç¼©è¿›å±‚çº§
                    depth = relative_path.count(os.sep)
                    indent = "  " * depth
                    file_name = os.path.basename(relative_path)
                    file_structure.append(f"{indent}â”œâ”€ {file_name} ({size_str})")
                
                structure_text = "\n".join(file_structure)
                return structure_text, None
            
            with open(preview_file_path, 'rb') as f:
                content_bytes = f.read(self.preview_length * 4)
            
            preview_text_raw, encoding = self._get_preview_from_bytes(content_bytes)
            
            inner_file_name = os.path.relpath(preview_file_path, extract_path)
            extra_info = f"å·²è§£å‹ã€Œ{inner_file_name}ã€(æ ¼å¼ {encoding})"
            preview_text = f"{extra_info}\n{preview_text_raw}"
            
        except FileNotFoundError:
            logger.error("è§£å‹å¤±è´¥ï¼šå®¹å™¨å†…æœªæ‰¾åˆ° 7za å‘½ä»¤ã€‚è¯·å®‰è£… p7zip-fullã€‚")
            error_msg = "è§£å‹å¤±è´¥ï¼šæœªå®‰è£… 7za"
        except Exception as e:
            logger.error(f"å¤„ç†ZIPæ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            error_msg = "å¤„ç†å‹ç¼©æ–‡ä»¶æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯"
        finally:
            if os.path.exists(extract_path):
                asyncio.create_task(self._cleanup_folder(extract_path))
        
        return preview_text, error_msg

    async def _cleanup_folder(self, path: str):
        """å¼‚æ­¥æ¸…ç†æ–‡ä»¶å¤¹åŠå…¶å†…å®¹ã€‚"""
        await asyncio.sleep(5)
        try:
            for dirpath, dirnames, filenames in os.walk(path, topdown=False):
                for filename in filenames:
                    os.remove(os.path.join(dirpath, filename))
                for dirname in dirnames:
                    os.rmdir(os.path.join(dirpath, dirname))
            os.rmdir(path)
            logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹: {path}")
        except OSError as e:
            logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤¹ {path} å¤±è´¥: {e}")

    async def _get_file_preview(self, event: AstrMessageEvent, file_info: dict) -> tuple[str, str | None]:
        group_id = int(event.get_group_id())
        file_id = file_info.get("file_id")
        file_name = file_info.get("file_name", "")
        _, file_extension = os.path.splitext(file_name)
        
        is_txt = file_extension.lower() == '.txt'
        is_zip = self.enable_zip_preview and file_extension.lower() == '.zip'
        
        if not (is_txt or is_zip):
            return "", f"âŒ æ–‡ä»¶ã€Œ{file_name}ã€ä¸æ˜¯æ”¯æŒçš„æ–‡æœ¬æˆ–å‹ç¼©æ ¼å¼ï¼Œæ— æ³•é¢„è§ˆã€‚"
            
        logger.info(f"[{group_id}] æ­£åœ¨ä¸ºæ–‡ä»¶ '{file_name}' (ID: {file_id}) è·å–é¢„è§ˆ...")
        
        local_file_path = None
        
        try:
            client = event.bot
            url_result = await client.api.call_action('get_group_file_url', group_id=group_id, file_id=file_id)
            if not (url_result and url_result.get('url')):
                return "", f"âŒ æ— æ³•è·å–æ–‡ä»¶ã€Œ{file_name}ã€çš„ä¸‹è½½é“¾æ¥ã€‚"
            url = url_result['url']
        except ActionFailed as e:
            if e.result.get('retcode') == 1200:
                error_message = (
                    f"âŒ é¢„è§ˆæ–‡ä»¶ã€Œ{file_name}ã€å¤±è´¥ï¼š\n"
                    f"è¯¥æ–‡ä»¶å¯èƒ½å·²å¤±æ•ˆã€‚\n"
                    f"å»ºè®®ä½¿ç”¨ /df {os.path.splitext(file_name)[0]} å°†å…¶åˆ é™¤ã€‚"
                )
                return "", error_message
            else:
                return "", f"âŒ é¢„è§ˆå¤±è´¥ï¼ŒAPIè¿”å›é”™è¯¯ï¼š{e.result.get('wording', 'æœªçŸ¥é”™è¯¯')}"
        
        try:
            async with aiohttp.ClientSession() as session:
                async with self.download_semaphore:
                    range_header = None
                    if is_txt:
                        read_bytes_limit = self.preview_length * 4
                        range_header = {'Range': f'bytes=0-{read_bytes_limit - 1}'}
                    async with session.get(url, headers=range_header, timeout=30) as resp:
                        if resp.status != 200 and resp.status != 206:
                            return "", f"âŒ ä¸‹è½½æ–‡ä»¶ã€Œ{file_name}ã€å¤±è´¥ (HTTP: {resp.status})ã€‚"
                        
                        temp_dir = os.path.join(StarTools.get_data_dir('astrbot_plugin_GroupFS'), 'temp_file_previews')
                        os.makedirs(temp_dir, exist_ok=True)
                        local_file_path = os.path.join(temp_dir, f"{file_id}_{file_name}")
                        
                        content_bytes = await resp.read()
                        with open(local_file_path, 'wb') as f:
                            f.write(content_bytes)
            
            preview_content = ""
            error_msg = None
            if is_txt:
                decoded_text, _ = self._get_preview_from_bytes(content_bytes)
                preview_content = decoded_text
            elif is_zip:
                preview_text, error_msg = await self._get_preview_from_zip(local_file_path)
                if error_msg:
                    return "", error_msg
                preview_content = preview_text
            
            # æ–‡ä»¶ç»“æ„åˆ—è¡¨ä¸æˆªæ–­ï¼Œæ™®é€šæ–‡æœ¬é¢„è§ˆæ‰æˆªæ–­
            is_file_structure = preview_content.startswith("ğŸ“¦ å‹ç¼©åŒ…å†…æ–‡ä»¶ç»“æ„ï¼š")
            if not is_file_structure and len(preview_content) > self.preview_length:
                preview_content = preview_content[:self.preview_length] + "..."
            
            # å‘é€å‰è¾“å‡ºæ—¥å¿—
            logger.info(f"[æ–‡ä»¶é¢„è§ˆ] æ–‡ä»¶: {file_name}, é¢„è§ˆé•¿åº¦: {len(preview_content)}, æ˜¯å¦æ–‡ä»¶ç»“æ„: {is_file_structure}")
            
            return preview_content, None
                
        except asyncio.TimeoutError:
            return "", f"âŒ é¢„è§ˆæ–‡ä»¶ã€Œ{file_name}ã€è¶…æ—¶ã€‚"
        except Exception as e:
            logger.error(f"[{group_id}] è·å–æ–‡ä»¶ '{file_name}' é¢„è§ˆæ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}", exc_info=True)
            return "", f"âŒ é¢„è§ˆæ–‡ä»¶ã€Œ{file_name}ã€æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚"
        finally:
            if local_file_path and os.path.exists(local_file_path):
                try:
                    os.remove(local_file_path)
                    logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {local_file_path}")
                except OSError as e:
                    logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {local_file_path} å¤±è´¥: {e}")

    async def _create_zip_archive(self, source_dir: str, target_zip_path: str, password: str) -> bool:
        """ä½¿ç”¨å¤–éƒ¨å‘½ä»¤è¡Œå·¥å…· (7za) å‹ç¼©æ•´ä¸ªç›®å½•ã€‚"""
        VOLUME_SIZE = '512m' 
        try:
            dir_to_zip = os.path.basename(source_dir)
            parent_dir = os.path.dirname(source_dir)

            # 7za a -tzip: æ·»åŠ å¹¶åˆ›å»º zip æ ¼å¼å½’æ¡£
            # -r: é€’å½’
            command = ['7za', 'a', '-tzip', target_zip_path, dir_to_zip, '-r', f'-v{VOLUME_SIZE}']
            
            if password:
                # 7za ä½¿ç”¨ -p[å¯†ç ] æ ¼å¼
                command.append(f"-p{password}")
            
            logger.info(f"[ç¾¤æ–‡ä»¶å¤‡ä»½-å‹ç¼©] æ­£åœ¨æ‰§è¡Œå‹ç¼©å‘½ä»¤: {' '.join(command)}")
            
            process = await asyncio.create_subprocess_exec(
                *command,
                cwd=parent_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                error_message = stderr.decode('utf-8', errors='ignore')
                logger.error(f"[ç¾¤æ–‡ä»¶å¤‡ä»½-å‹ç¼©] æ‰“åŒ…å¤±è´¥ï¼Œè¿”å›ç  {process.returncode}: {error_message}")
                return False
            
            logger.info(f"[ç¾¤æ–‡ä»¶å¤‡ä»½-å‹ç¼©] æ‰“åŒ…æˆåŠŸ: {target_zip_path}")
            return True

        except FileNotFoundError:
            logger.error("[ç¾¤æ–‡ä»¶å¤‡ä»½-å‹ç¼©] å‹ç¼©å¤±è´¥ï¼šå®¹å™¨å†…æœªæ‰¾åˆ° 7za å‘½ä»¤ã€‚è¯·å®‰è£… p7zip-full æˆ– 7-Zipã€‚")
            return False
        except Exception as e:
            logger.error(f"[ç¾¤æ–‡ä»¶å¤‡ä»½-å‹ç¼©] æ‰“åŒ…æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return False

    async def _perform_group_file_backup(self, event: AstrMessageEvent, group_id: int, date_filter_timestamp: Optional[int] = None):
        log_prefix = f"[ç¾¤æ–‡ä»¶å¤‡ä»½-{group_id}]"
        backup_root_dir = None
        final_zip_path = None
        
        try:
            client = self.bot or event.bot
            
            # 1. é¢„é€šçŸ¥ï¼šè·å–ç¾¤æ–‡ä»¶ç³»ç»Ÿä¿¡æ¯
            logger.info(f"{log_prefix} æ­£åœ¨è·å–ç¾¤æ–‡ä»¶ç³»ç»ŸåŸå§‹ä¿¡æ¯...")
            system_info = await client.api.call_action('get_group_file_system_info', group_id=group_id)
            
            # è®°å½•åŸå§‹çš„ç³»ç»Ÿä¿¡æ¯å­—å…¸
            logger.info(f"{log_prefix} --- ç¾¤æ–‡ä»¶ç³»ç»ŸåŸå§‹ä¿¡æ¯ START ---")
            for key, value in system_info.items():
                # é’ˆå¯¹å¤§æ•´æ•°è¿›è¡Œæ ¼å¼åŒ–ï¼Œä¾¿äºé˜…è¯»
                if isinstance(value, int) and ('space' in key or 'count' in key):
                    logger.info(f"{log_prefix} | {key}: {value} ({utils.format_bytes(value)})")
                else:
                    logger.info(f"{log_prefix} | {key}: {value}")
            logger.info(f"{log_prefix} --- ç¾¤æ–‡ä»¶ç³»ç»ŸåŸå§‹ä¿¡æ¯ END ---")

            total_count = system_info.get('file_count', 'æœªçŸ¥')
            
            notification = (
                f"å¤‡ä»½ä»»åŠ¡å·²å¯åŠ¨ï¼Œç›®æ ‡ç¾¤ID: {group_id}ã€‚\n"
                f"è¯¥ç¾¤æ–‡ä»¶æ€»æ•°: {total_count}ã€‚"
            )
            if date_filter_timestamp:
                date_str = datetime.fromtimestamp(date_filter_timestamp).strftime('%Y-%m-%d')
                notification += f"\nå°†ä»…å¤‡ä»½ {date_str} ä¹‹åä¸Šä¼ çš„æ–‡ä»¶ã€‚"
            notification += "\nå¤‡ä»½æ“ä½œå°†éå†æ‰€æœ‰æ–‡ä»¶ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿã€‚"
            
            await event.send(MessageChain([Comp.Plain(notification)]))
            logger.info(f"{log_prefix} é¢„é€šçŸ¥å·²å‘é€ã€‚")

            # 2. å‡†å¤‡å·¥ä½œï¼šè·å–ç¾¤åã€åˆ›å»ºæœ¬åœ°ä¸´æ—¶ç›®å½•
            group_info = await client.api.call_action('get_group_info', group_id=group_id)
            group_name = group_info.get('group_name', str(group_id))
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            
            temp_plugin_dir = StarTools.get_data_dir('astrbot_plugin_GroupFS')
            temp_base_dir = os.path.join(temp_plugin_dir, 'temp_backup_cache') 
            
            # å®é™…å­˜æ”¾æ–‡ä»¶å’Œæœ€ç»ˆ zip çš„ç›®å½•
            backup_root_dir = os.path.join(temp_base_dir, f"{group_name}") 
            
            # åˆ›å»ºç›®å½•
            os.makedirs(backup_root_dir, exist_ok=True)
            logger.info(f"{log_prefix} æœ¬åœ°å¤‡ä»½ç›®å½•: {backup_root_dir}")

            # 3. é€’å½’è·å–æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
            all_files_info = await self._get_all_files_with_path(group_id, client)
            
            # 4. è¿‡æ»¤å’Œä¸‹è½½æ–‡ä»¶
            total_file_count_all = len(all_files_info)
            downloaded_files_count = 0
            downloaded_files_size = 0
            failed_downloads = []
            
            size_limit_bytes = self.backup_file_size_limit_mb * 1024 * 1024
            
            for i, file_info in enumerate(all_files_info):
                file_name = file_info.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                file_id = file_info.get('file_id')
                file_size = file_info.get('size', 0)
                file_modify_time = file_info.get('modify_time', 0)
                relative_path = file_info.get('relative_path', '')
                
                # 4.1. è¿‡æ»¤ï¼šæ—¥æœŸç­›é€‰
                if date_filter_timestamp and file_modify_time < date_filter_timestamp:
                    continue

                # 4.2. è¿‡æ»¤ï¼šå¤§å°å’Œåç¼€å
                if size_limit_bytes > 0 and file_size > size_limit_bytes:
                    continue
                
                _, ext = os.path.splitext(file_name)
                ext = ext[1:].lower()
                if self.backup_file_extensions and ext not in self.backup_file_extensions:
                    continue
                
                # 4.3. ä¸‹è½½
                download_success = await self._download_and_save_file(
                    group_id, file_id, file_name, file_size, relative_path, backup_root_dir, client
                )
                
                if download_success:
                    downloaded_files_count += 1
                    downloaded_files_size += file_size
                else:
                    failed_downloads.append(file_name)

                #await asyncio.sleep(0.5) # ä¸‹è½½é—´éš”

            # 5. å‹ç¼©æ•´ä¸ªç›®å½•
            final_zip_name = f"{group_name}_å¤‡ä»½_{timestamp}.zip"
            final_zip_path = os.path.join(temp_base_dir, final_zip_name)
            
            logger.info(f"{log_prefix} æ–‡ä»¶ä¸‹è½½å®Œæˆï¼Œå…±æˆåŠŸä¸‹è½½ {downloaded_files_count} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å‹ç¼©...")

            zip_success = False

            if downloaded_files_count > 0:
                 zip_success = await self._create_zip_archive(backup_root_dir, final_zip_path, self.backup_zip_password)
            else:
                logger.warning(f"{log_prefix} æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶éœ€è¦å¤‡ä»½ï¼Œè·³è¿‡å‹ç¼©ã€‚")
                
            # 6. å‘é€å’Œæ¸…ç†
            if zip_success:
                
                temp_base_dir = os.path.dirname(final_zip_path)
                # åŸºç¡€åï¼šä¸åŒ…å« .zip éƒ¨åˆ† (å¦‚ 'botæµ‹è¯•_å¤‡ä»½_20251003_134542')
                zip_base_name_no_ext = os.path.basename(final_zip_path).rsplit('.zip', 1)[0]
                
                all_volumes = []
                
                logger.info(f"{log_prefix} æ­£åœ¨æŸ¥æ‰¾æ‰€æœ‰åˆ†å·æ–‡ä»¶ï¼ŒåŸºç¡€å: {zip_base_name_no_ext}ï¼Œç›®å½•: {temp_base_dir}")
                
                # æŸ¥æ‰¾æ‰€æœ‰åˆ†å·ï¼šåŒ¹é… 'åŸºç¡€å' + '.zip' + '.' + æ•°å­—
                for f in os.listdir(temp_base_dir):
                    logger.debug(f"{log_prefix} ç›®å½•é¡¹: {f}")
                    f_path = os.path.join(temp_base_dir, f)
                    if f.startswith(zip_base_name_no_ext) and (f.endswith('.zip') or f.split('.')[-1].isdigit()):
                         # ç¡®ä¿æˆ‘ä»¬åªæ·»åŠ ä¸»æ–‡ä»¶å’Œåˆ†å·æ–‡ä»¶ï¼Œæ’é™¤å…¶ä»–æ— å…³æ–‡ä»¶
                         if f == os.path.basename(final_zip_path) or f.startswith(f"{zip_base_name_no_ext}.zip."):
                            all_volumes.append(f_path)
                            try:
                                file_size = os.path.getsize(f_path)
                                logger.info(f"[{log_prefix}] è¯†åˆ«åˆ°åˆ†å·æ–‡ä»¶: {f} ({utils.format_bytes(file_size)})")
                            except Exception as e:
                                logger.warning(f"[{log_prefix}] è¯†åˆ«åˆ°åˆ†å·æ–‡ä»¶: {f} (è·å–å¤§å°å¤±è´¥: {e})")

                all_volumes.sort() # ç¡®ä¿æŒ‰é¡ºåºå‘é€
                
                is_single_volume = len(all_volumes) == 1
                
                logger.info(f"{log_prefix} æ‰¾åˆ°åˆ†å·æ–‡ä»¶æ•°: {len(all_volumes)}")
                
                if is_single_volume:
                    original_path = all_volumes[0]
                    original_name = os.path.basename(original_path)
                    
                    new_volume_name = f"{zip_base_name_no_ext}.zip"
                    new_volume_path = os.path.join(temp_base_dir, new_volume_name)
                    
                    os.rename(original_path, new_volume_path) # æ‰§è¡Œé‡å‘½å
                    all_volumes = [new_volume_path] # æ›´æ–°åˆ—è¡¨ä¸ºæ–°çš„è·¯å¾„
                    
                    logger.info(f"{log_prefix} [é‡å‘½å] å•åˆ†å·é‡å‘½åæˆåŠŸ: '{original_name}' -> '{new_volume_name}'")

                if not all_volumes:
                    # å¦‚æœå‹ç¼©æˆåŠŸï¼Œä½†ä¸€ä¸ªæ–‡ä»¶éƒ½æ²¡æ‰¾åˆ°ï¼Œè¯´æ˜è·¯å¾„æˆ–åŒ¹é…æœ‰é—®é¢˜
                    await event.send(MessageChain([Comp.Plain(f"âŒ å¤‡ä»½å‹ç¼©æˆåŠŸï¼Œä½†æœªåœ¨ç›®å½•ä¸­æ‰¾åˆ°ä»»ä½•ç”Ÿæˆçš„å‹ç¼©æ–‡ä»¶ï¼è¯·æ£€æŸ¥æ—¥å¿—ã€‚")]))
                    zip_success = False 
                    
                else:
                    # æ„é€ å›å¤æ¶ˆæ¯
                    reply_message = (
                        f"âœ… ç¾¤æ–‡ä»¶å¤‡ä»½å®Œæˆï¼\n"
                        f"æˆåŠŸå¤‡ä»½æ–‡ä»¶æ•°: {downloaded_files_count} ä¸ª (æ€»å¤§å°: {utils.format_bytes(downloaded_files_size)})\n"
                        f"{'å…±' if len(all_volumes) > 1 else ''} {len(all_volumes)} ä¸ªæ–‡ä»¶å³å°†å‘é€ï¼Œè¯·æ³¨æ„æ¥æ”¶ï¼"
                    )
                    if failed_downloads:
                        reply_message += f"\nâš ï¸ å¤‡ä»½å¤±è´¥æ–‡ä»¶æ•°: {len(failed_downloads)} ä¸ª (è¯¦è§æ—¥å¿—)"
                    
                    await event.send(MessageChain([Comp.Plain(reply_message)]))

                    # é€ä¸ªå‘é€åˆ†å·æ–‡ä»¶
                    all_sent_success = True
                    for volume_path in all_volumes:
                        volume_name = os.path.basename(volume_path)
                        logger.info(f"{log_prefix} æ­£åœ¨å‘é€åˆ†å·: {volume_name}...")
                        
                        if not await self._upload_and_send_file_via_api(event, volume_path, volume_name):
                            all_sent_success = False
                            # å‘é€å¤±è´¥é€šçŸ¥ç»™ç”¨æˆ·
                            await event.send(MessageChain([Comp.Plain(f"âŒ æ–‡ä»¶ {volume_name} å‘é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥ Bot é…ç½®ã€‚")]))
                            break
                            
                    if not all_sent_success:
                        await event.send(MessageChain([Comp.Plain(f"âŒ å¤‡ä»½å‘é€ä¸­æ–­ã€‚è¯·æ£€æŸ¥æ—¥å¿—ã€‚")]))
                    
                        
            elif downloaded_files_count == 0:
                 await event.send(MessageChain([Comp.Plain(f"â„¹ï¸ å¤‡ä»½ä»»åŠ¡å®Œæˆã€‚ä½†æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆå¤§å°æˆ–åç¼€åé™åˆ¶çš„ä»»ä½•æ–‡ä»¶ã€‚")]))
            else:
                await event.send(MessageChain([Comp.Plain(f"âŒ å¤‡ä»½ä»»åŠ¡å¤±è´¥ï¼šå‹ç¼©æ–‡ä»¶å¤±è´¥æˆ–æ‰¾ä¸åˆ°å‹ç¼©åŒ…ã€‚è¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚")]))
            
        except Exception as e:
            logger.error(f"{log_prefix} å¤‡ä»½ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}", exc_info=True)
            await event.send(MessageChain([Comp.Plain(f"âŒ å¤‡ä»½ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œå‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚è¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚")]))
        finally:
             asyncio.create_task(self._cleanup_backup_temp(backup_root_dir, final_zip_path))

    @filter.command("ddf")
    async def on_detect_duplicates_command(self, event: AstrMessageEvent):
        """æ£€æµ‹ç¾¤æ–‡ä»¶ä¸­çš„é‡å¤æ–‡ä»¶ï¼ˆä½¿ç”¨LLMåˆ†æï¼‰"""
        if not self.bot:
            self.bot = event.bot
        
        user_id = int(event.get_sender_id())
        group_id = event.get_group_id()
        
        if not group_id:
            yield event.plain_result("âŒ æ­¤æŒ‡ä»¤ä»…å¯åœ¨ç¾¤èŠä¸­ä½¿ç”¨ã€‚")
            return
        
        # æƒé™æ ¡éªŒ
        if user_id not in self.admin_users:
            yield event.plain_result("âš ï¸ æ‚¨æ²¡æœ‰æ‰§è¡Œé‡å¤æ–‡ä»¶æ£€æµ‹çš„æƒé™ã€‚")
            return
        
        # ç™½åå•æ ¡éªŒ
        if self.group_whitelist and int(group_id) not in self.group_whitelist:
            yield event.plain_result("âš ï¸ å½“å‰ç¾¤èŠä¸åœ¨æ’ä»¶é…ç½®çš„ç™½åå•ä¸­ã€‚")
            return
        
        try:
            # è·å–æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
            all_files = await self._get_all_files_recursive_core(int(group_id), self.bot)
            
            if not all_files:
                yield event.plain_result("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶ã€‚")
                return
            
            # æ„å»ºæ–‡ä»¶åˆ—è¡¨å­—ç¬¦ä¸²
            file_list_parts = []
            for file_info in all_files:
                file_name = file_info.get('file_name', 'æœªçŸ¥')
                file_size = file_info.get('size', 0)
                file_time = file_info.get('modify_time', 0)
                
                # æ ¼å¼åŒ–æ—¶é—´
                try:
                    time_str = datetime.fromtimestamp(file_time).strftime('%Y-%m-%d')
                except:
                    time_str = 'æœªçŸ¥'
                
                # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
                size_str = utils.format_bytes(file_size)
                
                file_list_parts.append(f"- {file_name} | {time_str} | {size_str}")
            
            file_list_str = "\n".join(file_list_parts)
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""è¯·ä»”ç»†åˆ†æä¸‹é¢çš„æ–‡ä»¶åˆ—è¡¨ï¼Œæ‰¾å‡ºæ‰€æœ‰ç–‘ä¼¼é‡å¤çš„å°è¯´æ–‡ä»¶ï¼ˆåŒä¸€æœ¬ä¹¦çš„ä¸åŒç‰ˆæœ¬ï¼‰ï¼Œå¹¶ä»¥ Markdown è¡¨æ ¼å½¢å¼åˆ—å‡ºã€‚

ã€é‡è¦è§„åˆ™ã€‘ï¼š
1. åªè¾“å‡ºé‡å¤æ–‡ä»¶å¯¹ï¼Œæ¯å¯¹æ–‡ä»¶åªå‡ºç°ä¸€æ¬¡ï¼åŒä¸€å¯¹æ–‡ä»¶ä¸è¦é‡å¤åˆ—å‡ºï¼ˆä¾‹å¦‚Aå’ŒBé‡å¤ï¼Œåªèƒ½å‡ºç°ä¸€æ¬¡ï¼Œä¸è¦æ—¢åˆ—"Aâ†’B"åˆåˆ—"Bâ†’A"ï¼‰
2. åˆ¤æ–­æ ‡å‡†ï¼š
   - æ–‡ä»¶åé«˜åº¦ç›¸ä¼¼ï¼ˆå»é™¤ç‰ˆæœ¬å·ã€æ—¥æœŸã€æ–‡ä»¶åç¼€ç­‰åä¸»ä½“åç§°ç›¸åŒï¼‰
   - åŒä¸€ä½œè€…çš„åŒä¸€ä½œå“
3. çŠ¶æ€åˆ—å¡«å†™ï¼š
   - "å¯åˆ é™¤"ï¼šé€šè¿‡æ–‡ä»¶åå·®å¼‚å¯æ˜ç¡®æ˜¯æ—§ç‰ˆæœ¬ï¼Œåº”è¯¥åˆ é™¤
   - "å¾…å®š"ï¼šæ— æ³•ç¡®å®šå“ªä¸ªæ˜¯æ—§ç‰ˆæœ¬ï¼Œéœ€è¦äººå·¥åˆ¤æ–­

ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
- è¡¨æ ¼åªæœ‰3åˆ—ï¼šçŠ¶æ€ | æ—§ç‰ˆæœ¬æ–‡ä»¶ä¿¡æ¯ | æ–°ç‰ˆæœ¬æ–‡ä»¶ä¿¡æ¯
- æ–‡ä»¶ä¿¡æ¯å¿…é¡»åœ¨åŒä¸€ä¸ªå•å…ƒæ ¼å†…ï¼Œæ ¼å¼ä¸ºï¼šæ–‡ä»¶å \ ä¿®æ”¹æ—¶é—´ \ å¤§å°ï¼Œæ­¤ä¸‰è€…ç¼ºä¸€ä¸å¯
- æ³¨æ„ï¼šæ–‡ä»¶ä¿¡æ¯çš„å„éƒ¨åˆ†ç”¨"\"ï¼ˆåæ–œæ ï¼‰è¿æ¥ï¼Œä¿æŒåœ¨åŒä¸€ä¸ªè¡¨æ ¼å•å…ƒæ ¼ä¸­
- æŒ‰æ—§ç‰ˆæœ¬æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´ä»æ—§åˆ°æ–°æ’åº

ã€è¾“å‡ºç¤ºä¾‹ã€‘ï¼š
| çŠ¶æ€ | æ—§ç‰ˆæœ¬æ–‡ä»¶ä¿¡æ¯ | æ–°ç‰ˆæœ¬æ–‡ä»¶ä¿¡æ¯ |
|---|---|---|
| å¯åˆ é™¤ | ã€Šç¤ºä¾‹å°è¯´ã€‹01_10ç« .txt \ 2024-01-15 \ 1.2MB | ã€Šç¤ºä¾‹å°è¯´ã€‹01_20ç« .txt \ 2024-02-20 \ 2.5MB |
| å¾…å®š | ã€Šå¦ä¸€æœ¬ä¹¦ã€‹å®Œæ•´ç‰ˆ.zip \ 2024-03-10 \ 3.5MB | ã€Šå¦ä¸€æœ¬ä¹¦ã€‹å®Œæ•´ç‰ˆ.txt \ 2024-03-10 \ 8.2MB |

é‡è¦ï¼šè¡¨æ ¼åªèƒ½æœ‰3åˆ—ï¼Œä¸è¦æŠŠæ—¶é—´å’Œå¤§å°æ‹†åˆ†æˆç‹¬ç«‹çš„åˆ—ï¼

ã€ç‰¹åˆ«æ³¨æ„ã€‘ï¼š
- å¦‚æœæ–‡ä»¶åˆ—è¡¨ä¸­æ²¡æœ‰é‡å¤æ–‡ä»¶ï¼Œç›´æ¥è¾“å‡ºï¼šæœªæ£€æµ‹åˆ°é‡å¤æ–‡ä»¶
- ä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–è§£é‡Šæˆ–è¯¢é—®

ä»¥ä¸‹æ˜¯æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨ï¼š

{file_list_str}"""
            
            # è·å–å½“å‰èŠå¤©æ¨¡å‹
            umo = event.unified_msg_origin
            provider_id = await self.context.get_current_chat_provider_id(umo=umo)
            
            if not provider_id:
                yield event.plain_result("âŒ æœªé…ç½®èŠå¤©æ¨¡å‹ï¼Œæ— æ³•ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
                return
            
            yield event.plain_result("ğŸ¤– æ­£åœ¨è°ƒç”¨AIæ£€æµ‹é‡å¤æ–‡ä»¶ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
            
            # è°ƒç”¨LLM
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id,
                prompt=prompt,
            )
            
            if not llm_resp or not llm_resp.completion_text:
                yield event.plain_result("âŒ AIåˆ†æå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚")
                return
            
            # æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°é‡å¤æ–‡ä»¶
            response_text = llm_resp.completion_text.strip()
            if "æœªæ£€æµ‹åˆ°é‡å¤æ–‡ä»¶" in response_text:
                # å¦‚æœæ²¡æœ‰é‡å¤æ–‡ä»¶ï¼Œç›´æ¥å‘é€æ–‡æœ¬æ¶ˆæ¯
                yield event.plain_result("âœ… æœªæ£€æµ‹åˆ°é‡å¤æ–‡ä»¶")
                return
            
            # å°†æ–‡æœ¬è½¬æ¢ä¸ºå›¾ç‰‡
            try:
                logger.info(f"[é‡å¤æ–‡ä»¶æ£€æµ‹] AIå“åº”ç»“æœï¼š\n{response_text}")
                image_url = await self.text_to_image(response_text)
                yield event.image_result(image_url)
            except Exception as img_error:
                logger.warning(f"æ–‡æœ¬è½¬å›¾ç‰‡å¤±è´¥: {img_error}ï¼Œå°†å‘é€çº¯æ–‡æœ¬ç»“æœ")
                # å¦‚æœè½¬å›¾å¤±è´¥ï¼Œé™çº§ä¸ºå‘é€æ–‡æœ¬
                await self._send_or_forward(event, response_text, "é‡å¤æ–‡ä»¶æ£€æµ‹")
            
        except Exception as e:
            logger.error(f"æ£€æµ‹é‡å¤æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            yield event.plain_result(f"âŒ æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

    @filter.command("gfb")
    async def on_group_file_backup_command(self, event: AstrMessageEvent):
        if not self.bot: self.bot = event.bot
        
        # 1. è§£æç›®æ ‡ç¾¤IDå’Œæ—¥æœŸå‚æ•°
        group_id_str = event.get_group_id()
        user_id = int(event.get_sender_id())
        
        command_parts = event.message_str.split()
        target_group_id: Optional[int] = None
        date_filter_timestamp: Optional[int] = None
        
        # è§£æå‚æ•°: /gfb [ç¾¤å·] [æ—¥æœŸ]
        if len(command_parts) > 1:
            try:
                target_group_id = int(command_parts[1])
                # å¦‚æœæœ‰ç¬¬ä¸‰ä¸ªå‚æ•°ï¼Œå°è¯•è§£æä¸ºæ—¥æœŸ
                if len(command_parts) > 2:
                    date_filter_timestamp = utils.parse_date_param(command_parts[2])
                    if date_filter_timestamp is None:
                        await event.send(MessageChain([Comp.Plain("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ã€‚æ”¯æŒæ ¼å¼: YYYY-MM-DD, YYYYMMDD, YYYY/MM/DD\nç¤ºä¾‹: /gfb 123456 2024-01-01")]))
                        return
            except ValueError:
                # å¯èƒ½ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æ—¥æœŸè€Œä¸æ˜¯ç¾¤å·
                if group_id_str:
                    target_group_id = int(group_id_str)
                    date_filter_timestamp = utils.parse_date_param(command_parts[1])
                    if date_filter_timestamp is None:
                        await event.send(MessageChain([Comp.Plain("âŒ æ ¼å¼é”™è¯¯ï¼šè¯·æä¾›æœ‰æ•ˆçš„ç¾¤å·æˆ–æ—¥æœŸã€‚\nç”¨æ³•: /gfb [ç¾¤å·] [æ—¥æœŸ]\næ—¥æœŸæ ¼å¼: YYYY-MM-DD, YYYYMMDD, YYYY/MM/DD")]))
                        return
                else:
                    await event.send(MessageChain([Comp.Plain("âŒ æ ¼å¼é”™è¯¯ï¼šè¯·æä¾›æœ‰æ•ˆçš„ç¾¤å·ã€‚ç”¨æ³•: /gfb <ç¾¤å·> [æ—¥æœŸ]")]))
                    return
        elif group_id_str:
            # ç¾¤èŠä¸­ä¸”æ²¡æœ‰å‚æ•°ï¼Œå¤‡ä»½å½“å‰ç¾¤
            target_group_id = int(group_id_str)
        else:
            # ç§èŠä¸­ä¸”æ²¡æœ‰å‚æ•°
            await event.send(MessageChain([Comp.Plain("âŒ æ ¼å¼é”™è¯¯ï¼šåœ¨ç§èŠä¸­è¯·æŒ‡å®šè¦å¤‡ä»½çš„ç¾¤å·ã€‚\nç”¨æ³•: /gfb <ç¾¤å·> [æ—¥æœŸ]\næ—¥æœŸæ ¼å¼: YYYY-MM-DD, YYYYMMDD, YYYY/MM/DD")]))
            return

        logger.info(f"ç”¨æˆ· {user_id} è§¦å‘ /gfb å¤‡ä»½æŒ‡ä»¤ï¼Œç›®æ ‡ç¾¤ID: {target_group_id}, æ—¥æœŸç­›é€‰: {command_parts[2] if len(command_parts) > 2 else 'æ— '}")

        # 2. æƒé™å’Œç™½åå•æ ¡éªŒ
        if user_id not in self.admin_users:
            await event.send(MessageChain([Comp.Plain("âš ï¸ æ‚¨æ²¡æœ‰æ‰§è¡Œç¾¤æ–‡ä»¶å¤‡ä»½æ“ä½œçš„æƒé™ã€‚")]))
            return
        
        if self.group_whitelist and target_group_id not in self.group_whitelist:
            await event.send(MessageChain([Comp.Plain("âš ï¸ ç›®æ ‡ç¾¤èŠä¸åœ¨æ’ä»¶é…ç½®çš„ç™½åå•ä¸­ï¼Œæ“ä½œå·²æ‹’ç»ã€‚")]))
            return

        # 3. å¯åŠ¨å¼‚æ­¥å¤‡ä»½ä»»åŠ¡
        self.active_tasks.append(asyncio.create_task(
            self._perform_group_file_backup(event, target_group_id, date_filter_timestamp)
        ))
        event.stop_event()

    async def terminate(self):
        logger.info("æ’ä»¶ [ç¾¤æ–‡ä»¶ç³»ç»ŸGroupFS] æ­£åœ¨å¸è½½ï¼Œå–æ¶ˆæ‰€æœ‰ä»»åŠ¡...")

        if self.scheduler and self.scheduler.running:
            try:
                self.scheduler.shutdown(wait=False) 
                logger.info("APScheduler å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²æˆåŠŸåœæ­¢ã€‚")
            except Exception as e:
                logger.error(f"åœæ­¢ APScheduler æ—¶å‘ç”Ÿé”™è¯¯: {e}")

        for task in self.active_tasks:
            if not task.done():
                task.cancel()
        
        try:
            await asyncio.gather(*self.active_tasks, return_exceptions=True)
        except asyncio.CancelledError:
            pass
        
        logger.info("æ’ä»¶ [ç¾¤æ–‡ä»¶ç³»ç»ŸGroupFS] å·²å¸è½½ã€‚")